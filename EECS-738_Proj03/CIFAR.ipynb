{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Grayscale, Compose\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset are downloaded through PyTorch. The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. To make the prediction easier, I downloaded the grayscale of the image and convert the image data into numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "training_data = datasets.CIFAR10(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=Compose([Grayscale(),ToTensor()])\n",
    ")\n",
    "\n",
    "testing_data = datasets.CIFAR10(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=Compose([Grayscale(),ToTensor()])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset CIFAR10\n",
       "    Number of datapoints: 50000\n",
       "    Root location: data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               Grayscale(num_output_channels=1)\n",
       "               ToTensor()\n",
       "           )"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset CIFAR10\n",
       "    Number of datapoints: 10000\n",
       "    Root location: data\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               Grayscale(num_output_channels=1)\n",
       "               ToTensor()\n",
       "           )"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.2392, 0.1725, 0.1882,  ..., 0.5373, 0.5098, 0.5020],\n",
       "          [0.0706, 0.0000, 0.0392,  ..., 0.3686, 0.3529, 0.3686],\n",
       "          [0.0902, 0.0314, 0.1216,  ..., 0.3529, 0.3529, 0.3137],\n",
       "          ...,\n",
       "          [0.6745, 0.6000, 0.6118,  ..., 0.5216, 0.1373, 0.1490],\n",
       "          [0.5725, 0.5020, 0.5608,  ..., 0.5961, 0.2706, 0.2314],\n",
       "          [0.5882, 0.5333, 0.5725,  ..., 0.7373, 0.4824, 0.3843]]]), 6)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each instance (image) in the dataset is a 32x32 tensor matrix. To training this data, I define each pixel as one feature of each instance. Then the first step is to flatten the matrix to vector, so that each instance has 1024 features. I only used 10% of the training and testing data so the model will not run too long time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 1024)\n",
      "(1000, 1024)\n"
     ]
    }
   ],
   "source": [
    "training_sample = np.array([x_train[0].numpy().flatten() for x_train in training_data])\n",
    "testing_sample = np.array([x_test[0].numpy().flatten() for x_test in testing_data])\n",
    "\n",
    "training_sample_5k = training_sample[:5000]\n",
    "testing_sample_1k = testing_sample[:1000]\n",
    "\n",
    "print(training_sample_5k.shape)\n",
    "print(testing_sample_1k.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The label of this dataset contains 10 classes, so the neural network model of this dataset will have 10 nodes in output layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000,)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "training_label = np.array([y_train[-1] for y_train in training_data])\n",
    "testing_label = np.array([y_test[-1] for y_test in testing_data])\n",
    "\n",
    "training_label_5k = training_label[:5000]\n",
    "testing_label_1k = testing_label[:1000]\n",
    "\n",
    "print(training_label_5k.shape)\n",
    "print(testing_label_1k.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create possible active functionsand direvatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):  \n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_d(x):\n",
    "    return sigmoid(x) * (1-sigmoid(x))\n",
    "\n",
    "def tanh(x):\n",
    "    return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
    "\n",
    "def tanh_d(x):\n",
    "    return 1 - tanh(x)**2\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def relu_d(x):\n",
    "    x[x<0] = 0\n",
    "    x[x>=0] = 1\n",
    "    return x\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    exp = np.exp(x)\n",
    "    scores = exp / exp.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First randomly initialize weights and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization(num_input, num_hidden, num_output):\n",
    "    parameters = {}\n",
    "    \n",
    "    parameters['weight_hidden'] = np.random.rand(num_input,num_hidden)\n",
    "    parameters['weight_output'] = np.random.rand(num_hidden,num_output)\n",
    "    \n",
    "    parameters['bias_hidden'] = np.zeros((1, num_hidden))\n",
    "    parameters['bias_output'] = np.zeros((1, num_output))\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activate(f, activation):\n",
    "    if activation == 'sigmoid':\n",
    "        a = sigmoid(f)\n",
    "    elif activation == 'tanh':\n",
    "        a = tanh(f)\n",
    "    elif activation == 'relu':\n",
    "        a = relu(f)\n",
    "        \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activate_derivative(f, activation):\n",
    "    if activation == 'sigmoid':\n",
    "        a_d = sigmoid_d(f)\n",
    "    elif activation == 'tanh':\n",
    "        a_d = tanh_d(f)\n",
    "    elif activation == 'relu':\n",
    "        a_d = relu_d(f)\n",
    "\n",
    "    return a_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this dataset, I want to define a model that use same activation funciton on hidden layer and output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x, parameters, activation):\n",
    "    predict = {}\n",
    "    \n",
    "    predict['input_hidden'] = np.dot(x, parameters['weight_hidden']) + parameters['bias_hidden']\n",
    "    predict['output_hidden'] = activate(predict['input_hidden'], activation)\n",
    "        \n",
    "    predict['input_output'] = np.dot(predict['output_hidden'], parameters['weight_output']) + parameters['bias_output']\n",
    "    predict['predict_y'] = activate(predict['input_output'], activation)\n",
    "    \n",
    "    return predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple model that only contains one hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(x, y, parameters, predict, learning_rate, activation):\n",
    "\n",
    "    # weight and bias update on output layer\n",
    "    diff = predict['predict_y'] - y\n",
    "    activate_dout = activate_derivative(predict['input_output'],activation)\n",
    "    dw_output = np.dot(predict['output_hidden'].T, activate_dout* diff)\n",
    "    db_output = np.sum(activate_dout*diff, axis=0,keepdims=True)\n",
    "    \n",
    "    # weight and bias update on hidden layer\n",
    "    p2 = np.dot(diff * activate_dout,parameters['weight_output'].T)\n",
    "    activate_dhidden = activate_derivative(predict['input_hidden'],activation)\n",
    "    dw_hidden = np.dot(x.T, activate_dhidden * p2)\n",
    "    db_hidden = np.sum(p2*activate_dhidden, axis=0,keepdims=True)\n",
    "    \n",
    "    parameters['weight_hidden'] -= learning_rate * dw_hidden\n",
    "    parameters['weight_output'] -= learning_rate * dw_output\n",
    "    parameters['bias_hidden'] -= learning_rate * db_hidden\n",
    "    parameters['bias_output'] -= learning_rate * db_output\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate neural network model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create the neural network model. For this model, I used Mean Square Error (MSE) as the cost funciton\n",
    "\n",
    "MSE: $\\dfrac{1}{2m} \\sum^{m}_{1} (\\hat{Y} - Y)^{2}$\n",
    "\n",
    "derivative of MSE: $\\hat{Y} - Y$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_model(x, y, hidden_width, activation, learning_rate, epochs):\n",
    "    parameter = initialization(x.shape[1], hidden_width, y.shape[1])\n",
    "#     parameter = initialization(x.shape[1], hidden_width, 1)\n",
    "    MSE = []\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        prediction = forward(x,parameter,activation)\n",
    "        \n",
    "        mse = ((1/2) * np.power((prediction['predict_y'] - y),2)).mean()\n",
    "        MSE.append(mse)\n",
    "\n",
    "        parameter = backpropagation(x, y, parameter, prediction, learning_rate, activation)\n",
    "        \n",
    "        if epoch % 1000 == 0:\n",
    "            print(\"After %d epochs, MSE is now: %f\" % (epoch, mse))\n",
    "        \n",
    "    return parameter, prediction, MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make prediction on testing data and calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_model(x, y, parameter, hidden_width, activation):\n",
    "    prediction = forward(x, parameter, activation)\n",
    "    \n",
    "    predict_y = np.argmax(prediction['predict_y'],axis=1)\n",
    "    y_label = np.argmax(y,axis=1)\n",
    "    correct_predictions = np.sum(predict_y == y_label)\n",
    "    accuracy = correct_predictions / len(y_label)\n",
    "    return accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 10)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "label_training = np.array(pd.get_dummies(training_label_5k))\n",
    "\n",
    "label_training.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 epochs, MSE is now: 0.449840\n",
      "After 1000 epochs, MSE is now: 0.287873\n",
      "After 2000 epochs, MSE is now: 0.167461\n",
      "After 3000 epochs, MSE is now: 0.167456\n",
      "After 4000 epochs, MSE is now: 0.167319\n",
      "After 5000 epochs, MSE is now: 0.126239\n",
      "After 6000 epochs, MSE is now: 0.126235\n",
      "After 7000 epochs, MSE is now: 0.126116\n",
      "After 8000 epochs, MSE is now: 0.085526\n",
      "After 9000 epochs, MSE is now: 0.085525\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameter,prediction,MSE = NN_model(training_sample_5k, label_training, 20, 'sigmoid', 0.0001, 10000)\n",
    "x = np.arange(0,10000)\n",
    "plt.figure()\n",
    "plt.plot(x,MSE)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction accuracy is: 0.109000\n"
     ]
    }
   ],
   "source": [
    "label_testing = np.array(pd.get_dummies(testing_label_1k))\n",
    "\n",
    "accuracy = predict_model(testing_sample_1k, label_testing, parameter, 20, 'sigmoid')\n",
    "print(\"The prediction accuracy is: %f\" % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this prediction model, Since the dataset contains large number of features, so it takes longer time to training the data. The cost plot also shows that the cost value is reduced during the training. But event the meas square error at the end is remains small, but the accuracy is much lower than the model for Abalone dataset. And this dataset contains more feature and more classes, which may proved my assumption that when we have more complex data (large number of features and multiple classifications), we need to have a complex model to increase the accuracy, eg. more hidden layer and more nodes in each hidden layer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
