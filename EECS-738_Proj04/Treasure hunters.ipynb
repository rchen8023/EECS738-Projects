{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate reward table for reinforcement learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_reward_map(game_map):\n",
    "    reward_map = game_map.copy()\n",
    "    \n",
    "    reward_map[reward_map=='blank'] = -0.1\n",
    "    reward_map[reward_map=='rock'] = -5\n",
    "    reward_map[reward_map=='monster'] = -10\n",
    "    reward_map[reward_map=='treasure'] = 10\n",
    "    \n",
    "    reward_map = reward_map.astype(float)\n",
    "    \n",
    "    return reward_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The update_next_state function will update and check for next state, if the next state is out of range, (eg. current position at (0,0), but action is UP, or LEFT), then stay at current state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_next_state(current_state, action,maxedge):\n",
    "    next_state = current_state\n",
    "    if action == 0 :\n",
    "        next_state = (current_state[0] - 1, current_state[1])\n",
    "    elif action == 1 :\n",
    "        next_state = (current_state[0] + 1, current_state[1])\n",
    "    elif action == 2 :\n",
    "        next_state = (current_state[0], current_state[1] - 1)\n",
    "    elif action == 3 :\n",
    "        next_state = (current_state[0], current_state[1] + 1)\n",
    "    \n",
    "    # check if next state is out of range\n",
    "    if next_state[0] < 0 or next_state[1] < 0 or next_state[0] > maxedge[0]-1 or next_state[1] > maxedge[1]-1:\n",
    "        next_state = current_state\n",
    "\n",
    "    return next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now start Q_learning module for reinforcement learning\n",
    "\n",
    "$Q(S_t, A_t) = Q(S_t, A_t) + \\alpha [ \\, R_{t+1} + \\gamma \\max_{a} Q(S_{t+1}, a) - Q(S_t, A_t)] \\,$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_learning(alpha, gamma, start_state, end_state, reward_map, epochs):\n",
    "    n, p = reward_map.shape\n",
    "    \n",
    "    # initialize Q_table\n",
    "    Q_value = [0,0,0,0]\n",
    "    Q_table = np.zeros((n,p,4))\n",
    "\n",
    "    for i in range(epochs):\n",
    "        current_state = start_state\n",
    "        while current_state != end_state:\n",
    "            # randomly pick an action\n",
    "            action = random.randint(0,3)\n",
    "            # Q learning\n",
    "            next_state = update_next_state(current_state,action,(n,p))\n",
    "            Q_next = Q_table[next_state[0], next_state[1]]\n",
    "            reward_after_action = reward_map[next_state[0], next_state[1]]\n",
    "\n",
    "            old_Q = Q_table[current_state[0], current_state[1]][action]\n",
    "\n",
    "            new_Q = old_Q + alpha*(reward_after_action + gamma * max(Q_next) - old_Q)\n",
    "            Q_table[current_state[0], current_state[1]][action] = new_Q\n",
    "\n",
    "            current_state = next_state\n",
    "    return Q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on the Q_table after learning, find the optimal path that avoid obstacles and opponents with less step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_path(Q_table,start,end):\n",
    "    n, p, z= Q_table.shape\n",
    "    current = start\n",
    "    best_path = []\n",
    "    while current != end:\n",
    "        Q_value_current = Q_table[current[0],current[1]]\n",
    "        action = np.argmax(Q_value_current)\n",
    "        best_path.append(action)\n",
    "        next_state = update_next_state(current,action,(n,p))\n",
    "        current = next_state\n",
    "    return best_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement reinforcement learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rl_algorithm(game_map, start_point, alpha, gamma, epochs):\n",
    "\n",
    "    index = np.where(game_map == 'treasure')\n",
    "    index_tu = list(zip(index[0], index[1]))\n",
    "    end_point = index_tu[0]\n",
    "    \n",
    "    reward_map = generate_reward_map(game_map)\n",
    "    Q_table = Q_learning(alpha, gamma, start_point, end_point, reward_map, epochs)\n",
    "    best_path = get_best_path(Q_table,start_point, end_point)\n",
    "    \n",
    "    print(\"Done reinforcement learning!\")\n",
    "    print(\"Printing Q_table: \")\n",
    "    print(Q_table)\n",
    "    print(' ')\n",
    "    print(['The optimal path to treature from ', start_point, ' is: ', best_path])\n",
    "    print(\" \")\n",
    "    return Q_table, best_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### print out the original map with out optimal path\n",
    "\n",
    "\"-\" means blank cell  \n",
    "\"#\" is tree   \n",
    "\"X\" is monsters   \n",
    "\"$\" is treasure   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_graph(game_map):\n",
    "    graph = game_map.copy()\n",
    "    graph[graph=='blank'] = '-'\n",
    "    graph[graph=='rock'] = '#'\n",
    "    graph[graph=='monster'] = 'X'\n",
    "    graph[graph=='treasure'] = '$'\n",
    "    \n",
    "    print(\"Printing the map without path:\")\n",
    "    print(graph)\n",
    "    print(\" \")\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print out the map with optimal path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_graph_with_path(graph,best_path,current):\n",
    "    for action in best_path:\n",
    "        if action == 0:\n",
    "            graph[current[0],current[1]] = \"^\"\n",
    "        elif action == 1:\n",
    "            graph[current[0],current[1]] = \"v\"\n",
    "        elif action == 2:\n",
    "            graph[current[0],current[1]] = \"<\"\n",
    "        elif action == 3:\n",
    "            graph[current[0],current[1]] = \">\"\n",
    "\n",
    "        current = update_next_state(current,action,graph.shape)\n",
    "        \n",
    "    print(\"Printing the map with optimal path:\")\n",
    "    print(graph)\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing the map without path:\n",
      "[['-' '#' '$' '-' '-' '-']\n",
      " ['-' '#' '-' '#' '-' '-']\n",
      " ['-' '-' '-' '-' '#' '-']\n",
      " ['#' '-' 'X' '-' '-' '-']\n",
      " ['X' '-' '-' '#' '-' '#']\n",
      " ['-' '-' '#' '-' '-' '-']]\n",
      " \n",
      "Done reinforcement learning!\n",
      "Printing Q_table: \n",
      "[[[ 2.29999999  2.94064     2.3         3.        ]\n",
      "  [ 3.          1.32        2.3        10.        ]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 7.89998936  1.31999904  9.9999997   6.21994795]\n",
      "  [ 6.21998972  4.87598556  7.89999885  4.87599211]\n",
      "  [ 4.87599264  3.80079307  6.21999627  4.87599328]]\n",
      "\n",
      " [[ 2.29999999  3.8008      2.94064     1.32      ]\n",
      "  [ 3.          4.876       2.94064     7.9       ]\n",
      "  [10.          6.22        1.32        1.31999968]\n",
      "  [ 7.89999972  4.876       7.89999998  4.87599591]\n",
      "  [ 6.21999653 -1.0992      1.31999994  3.80079319]\n",
      "  [ 4.87599129  2.94063484  4.87599518  3.80079526]]\n",
      "\n",
      " [[ 2.94064    -1.95936     3.8008      4.876     ]\n",
      "  [ 1.32        3.8008      3.8008      6.22      ]\n",
      "  [ 7.9        -5.024       4.876       4.876     ]\n",
      "  [ 1.31999998  3.8008      6.22       -1.0992    ]\n",
      "  [ 4.87599491  2.94064     4.876       2.94062065]\n",
      "  [ 3.8007954   2.25251199 -1.0992      2.94063468]]\n",
      "\n",
      " [[ 3.8008     -7.647488   -1.95936     3.8008    ]\n",
      "  [ 4.876       2.94064    -1.95936    -5.024     ]\n",
      "  [ 6.22        2.252512    3.8008      3.8008    ]\n",
      "  [ 4.876      -1.95936    -5.024       2.94064   ]\n",
      "  [-1.0992      2.252512    3.8008      2.252512  ]\n",
      "  [ 2.94063536 -3.1979904   2.94064     2.252512  ]]\n",
      "\n",
      " [[-1.95936     1.7020096  -7.647488    2.94064   ]\n",
      "  [ 3.8008      2.252512   -7.647488    2.252512  ]\n",
      "  [-5.024      -3.1979904   2.94064    -1.95936   ]\n",
      "  [ 3.8008      1.26160768  2.252512    2.252512  ]\n",
      "  [ 2.94064     1.7020096  -1.95936    -3.1979904 ]\n",
      "  [ 2.252512    1.26160768  2.252512   -3.1979904 ]]\n",
      "\n",
      " [[-7.647488    1.7020096   1.7020096   2.252512  ]\n",
      "  [ 2.94064     2.252512    1.7020096  -3.1979904 ]\n",
      "  [ 2.252512   -3.1979904   2.252512    1.26160768]\n",
      "  [-1.95936     1.26160768 -3.1979904   1.7020096 ]\n",
      "  [ 2.252512    1.7020096   1.26160768  1.26160768]\n",
      "  [-3.1979904   1.26160768  1.7020096   1.26160768]]]\n",
      " \n",
      "['The optimal path to treature from ', (5, 0), ' is: ', [3, 0, 0, 0, 3, 0, 0]]\n",
      " \n",
      "Printing the map with optimal path:\n",
      "[['-' '#' '$' '-' '-' '-']\n",
      " ['-' '#' '^' '#' '-' '-']\n",
      " ['-' '>' '^' '-' '#' '-']\n",
      " ['#' '^' 'X' '-' '-' '-']\n",
      " ['X' '^' '-' '#' '-' '#']\n",
      " ['>' '^' '#' '-' '-' '-']]\n",
      " \n"
     ]
    }
   ],
   "source": [
    "blank = 'blank'\n",
    "rock = 'rock'\n",
    "monster = 'monster'\n",
    "treasure = 'treasure'\n",
    "start = (5,0)\n",
    "\n",
    "game_map = np.array([[blank, rock, treasure, blank, blank, blank],\n",
    "           [blank, rock, blank, rock, blank, blank],\n",
    "           [blank, blank, blank, blank, rock, blank],\n",
    "           [rock, blank, monster, blank, blank, blank],\n",
    "           [monster, blank, blank, rock, blank, rock],\n",
    "           [blank, blank, rock, blank, blank, blank]])\n",
    "\n",
    "graph = print_graph(game_map)\n",
    "\n",
    "Q_table, best_path = rl_algorithm(game_map,start,0.5,0.8,100)\n",
    "\n",
    "print_graph_with_path(graph,best_path,start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now find the best path from different entry point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing the map without path:\n",
      "[['-' '#' '$' '-' '-' '-']\n",
      " ['-' '#' '-' '#' '-' '-']\n",
      " ['-' '-' '-' '-' '#' '-']\n",
      " ['#' '-' 'X' '-' '-' '-']\n",
      " ['X' '-' '-' '#' '-' '#']\n",
      " ['-' '-' '#' '-' '-' '-']]\n",
      " \n",
      "Done reinforcement learning!\n",
      "Printing Q_table: \n",
      "[[[ 2.3         2.94063871  2.3         3.        ]\n",
      "  [ 3.          1.31999998  2.3        10.        ]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 7.88652344  1.31962101  9.99984741  6.08931519]\n",
      "  [ 6.16166534  4.86124613  7.89563637  4.84566641]\n",
      "  [ 4.85630265  3.78997946  6.20774376  4.79169927]]\n",
      "\n",
      " [[ 2.3         3.80079924  2.94063846  1.32      ]\n",
      "  [ 3.          4.87599902  2.9406339   7.9       ]\n",
      "  [10.          6.2199971   1.31999973  1.31997123]\n",
      "  [ 7.89957766  4.87599668  7.89999882  4.86922233]\n",
      "  [ 6.21286562 -1.09921645  1.31999383  3.79273571]\n",
      "  [ 4.85928539  2.93212394  4.87013897  3.79427441]]\n",
      "\n",
      " [[ 2.94063829 -1.95936153  3.80079974  4.87599983]\n",
      "  [ 1.31999999  3.80079848  3.80079944  6.21999986]\n",
      "  [ 7.8999999  -5.02400046  4.87599968  4.87599908]\n",
      "  [ 1.31999559  3.80079203  6.21999985 -1.09922086]\n",
      "  [ 4.87026801  2.94061583  4.87599296  2.93626004]\n",
      "  [ 3.79559067  2.2524694  -1.09922085  2.93507879]]\n",
      "\n",
      " [[ 3.80079951 -7.64767649 -1.95936275  3.80079827]\n",
      "  [ 4.87599903  2.94062409 -1.95936188 -5.02400054]\n",
      "  [ 6.21999976  2.25227523  3.80079598  3.80079706]\n",
      "  [ 4.87599809 -1.95942615 -5.0240009   2.94062381]\n",
      "  [-1.09922192  2.2524754   3.80078876  2.25249392]\n",
      "  [ 2.93468331 -3.19807475  2.94062254  2.2524768 ]]\n",
      "\n",
      " [[-1.95938106  1.70196504 -7.64754068  2.94060927]\n",
      "  [ 3.80078956  2.25229843 -7.6475537   2.25235935]\n",
      "  [-5.02400104 -3.19878679  2.94058926 -1.95948294]\n",
      "  [ 3.8007461   1.26016309  2.24973681  2.2524007 ]\n",
      "  [ 2.94061757  1.70193834 -1.95946766 -3.19801813]\n",
      "  [ 2.2524726   1.26103239  2.25249303 -3.19815698]]\n",
      "\n",
      " [[-7.64752372  1.70196754  1.70199191  2.25249308]\n",
      "  [ 2.94062502  2.2524127   1.70196262 -3.19817541]\n",
      "  [ 2.25103007 -3.19821163  2.25248582  1.26111355]\n",
      "  [-1.95952958  1.26117229 -3.19817547  1.70183132]\n",
      "  [ 2.25246802  1.70188747  1.26092788  1.26106253]\n",
      "  [-3.19802167  1.26090125  1.7014      1.26106133]]]\n",
      " \n",
      "['The optimal path to treature from ', (0, 0), ' is: ', [3, 3]]\n",
      " \n",
      "Printing the map with optimal path:\n",
      "[['>' '>' '$' '-' '-' '-']\n",
      " ['-' '#' '-' '#' '-' '-']\n",
      " ['-' '-' '-' '-' '#' '-']\n",
      " ['#' '-' 'X' '-' '-' '-']\n",
      " ['X' '-' '-' '#' '-' '#']\n",
      " ['-' '-' '#' '-' '-' '-']]\n",
      " \n"
     ]
    }
   ],
   "source": [
    "start = (0,0)\n",
    "graph = print_graph(game_map)\n",
    "\n",
    "Q_table, best_path = rl_algorithm(game_map,start,0.5,0.8,100)\n",
    "\n",
    "print_graph_with_path(graph,best_path,start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try with different map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing the map without path:\n",
      "[['-' '#' '#' '-' '-']\n",
      " ['-' '#' '-' '-' '-']\n",
      " ['-' '-' '-' '#' '-']\n",
      " ['-' '#' '-' '#' '-']\n",
      " ['-' 'X' '-' '-' '$']]\n",
      " \n",
      "Done reinforcement learning!\n",
      "Printing Q_table: \n",
      "[[[ 1.26160768  1.7020096   1.26160768 -3.99071386]\n",
      "  [-3.99071386 -2.647488    1.26160768 -2.647488  ]\n",
      "  [-2.647488    2.94064    -3.99071386  2.94064   ]\n",
      "  [ 2.94064     3.8008     -2.647488    3.8008    ]\n",
      "  [ 3.8008      4.876       2.94064     3.8008    ]]\n",
      "\n",
      " [[ 1.26160768  2.252512    1.7020096  -2.647488  ]\n",
      "  [-3.99071386  2.94064     1.7020096   2.94064   ]\n",
      "  [-2.647488    3.8008     -2.647488    3.8008    ]\n",
      "  [ 2.94064    -0.024       2.94064     4.876     ]\n",
      "  [ 3.8008      6.22        3.8008      4.876     ]]\n",
      "\n",
      " [[ 1.7020096   1.7020096   2.252512    2.94064   ]\n",
      "  [-2.647488   -1.0992      2.252512    3.8008    ]\n",
      "  [ 2.94064     4.876       2.94064    -0.024     ]\n",
      "  [ 3.8008      1.32        3.8008      6.22      ]\n",
      "  [ 4.876       7.9        -0.024       6.22      ]]\n",
      "\n",
      " [[ 2.252512    1.26160768  1.7020096  -1.0992    ]\n",
      "  [ 2.94064    -5.024       1.7020096   4.876     ]\n",
      "  [ 3.8008      6.22       -1.0992      1.32      ]\n",
      "  [-0.024       7.9         4.876       7.9       ]\n",
      "  [ 6.22       10.          1.32        7.9       ]]\n",
      "\n",
      " [[ 1.7020096   1.26160768  1.26160768 -5.024     ]\n",
      "  [-1.0992     -5.024       1.26160768  6.22      ]\n",
      "  [ 4.876       6.22       -5.024       7.9       ]\n",
      "  [ 1.32        7.9         6.22       10.        ]\n",
      "  [ 0.          0.          0.          0.        ]]]\n",
      " \n",
      "['The optimal path to treature from ', (0, 0), ' is: ', [1, 1, 3, 3, 1, 1, 3, 3]]\n",
      " \n",
      "Printing the map with optimal path:\n",
      "[['v' '#' '#' '-' '-']\n",
      " ['v' '#' '-' '-' '-']\n",
      " ['>' '>' 'v' '#' '-']\n",
      " ['-' '#' 'v' '#' '-']\n",
      " ['-' 'X' '>' '>' '$']]\n",
      " \n"
     ]
    }
   ],
   "source": [
    "start = (0,0)\n",
    "game_map = np.array([[blank, rock, rock, blank, blank],\n",
    "                    [blank, rock, blank, blank, blank],\n",
    "                    [blank, blank, blank, rock, blank],\n",
    "                    [blank, rock, blank, rock, blank],\n",
    "                    [blank, monster, blank, blank, treasure]])\n",
    "\n",
    "graph = print_graph(game_map)\n",
    "\n",
    "Q_table, best_path = rl_algorithm(game_map,start,0.5,0.8,100)\n",
    "\n",
    "print_graph_with_path(graph,best_path,start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
